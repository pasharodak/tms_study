<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>MathML Test</title>
<script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/mml-chtml.js">
</script>
</head>
<body>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
# Линейная регрессия на  6 строках: пошагово и с подстановкой чисел
 0.00632  18.00   2.310  0  0.5380  6.5750  65.20  4.0900   1  296.0  15.30 396.90   4.98  24.00
 0.02731   0.00   7.070  0  0.4690  6.4210  78.90  4.9671   2  242.0  17.80 396.90   9.14  21.60
 0.02729   0.00   7.070  0  0.4690  7.1850  61.10  4.9671   2  242.0  17.80 392.83   4.03  34.70
 0.03237   0.00   2.180  0  0.4580  6.9980  45.80  6.0622   3  222.0  18.70 394.63   2.94  33.40
 0.06905   0.00   2.180  0  0.4580  7.1470  54.20  6.0622   3  222.0  18.70 396.90   5.33  36.20
 0.02985   0.00   2.180  0  0.4580  6.4300  58.70  6.0622   3  222.0  18.70 394.12   5.21  28.70

Ниже я покажу ровно, как по формулам считаются стандартизация, градиенты и обновления весов на первых двух итерациях градиентного спуска — с подстановкой твоих чисел. Чтобы не распухнуть в вычислениях, подробно распишу bias и 3 показательные фичи (CRIM, RM, RAD). Для остальных признаки делаются идентично.

---

## Данные и обозначения

-  \(m=6\) объектов, \(n=13\) признаков, целевая в последнем столбце.
- Цель: минимизировать MSE и найти веса \(\mathbf{w}=[w_0,\dots,w_{13}]\), где \(w_0\) — bias.
- Модель:
\[
\hat{y}^{(i)} = w_0 + \sum_{j=1}^{13} w_j \, x^{(i)}_j
\]
- Градиентный спуск (batch):
\[
w_j \leftarrow w_j - \alpha \cdot \frac{2}{m}\sum_{i=1}^{m}\big(\hat{y}^{(i)}-y^{(i)}\big)\,x^{(i)}_j
\]
- Возьмём шаг \(\alpha=0.01\). Инициализация: \(w_j=0\).

---

## Подготовка признаков: стандартизация и bias

Стандартизируем каждый признак: \(z = \dfrac{x-\mu}{\sigma}\), где \(\mu\) — среднее по столбцу, \(\sigma\) — стандартное отклонение (с делением на \(m\), а не на \(m-1\)). Для bias добавляем столбец из единиц.

Покажу полностью для трёх признаков и укажу важное:

- Цель \(y = [24.0,\ 21.6,\ 34.7,\ 33.4,\ 36.2,\ 28.7]\).
- CHAS у всех строк равен 0 ⇒ при стандартизации его вклад равен 0.

#### Признак CRIM
- Значения: [0.00632, 0.02731, 0.02729, 0.03237, 0.06905, 0.02985]
- Среднее: \(\mu_{\text{CRIM}} \approx 0.032032\)
- СКО: \(\sigma_{\text{CRIM}} \approx 0.018624\)
- Z-оценки:
  \[
  z_{\text{CRIM}} \approx [-1.3806,\ -0.2535,\ -0.2546,\ 0.0182,\ 1.9877,\ -0.1171]
  \]

#### Признак RM
- Значения: [6.5750, 6.4210, 7.1850, 6.9980, 7.1470, 6.4300]
- Среднее: \(\mu_{\text{RM}} \approx 6.7927\)
- СКО: \(\sigma_{\text{RM}} \approx 0.3264\)
- Z-оценки:
  \[
  z_{\text{RM}} \approx [-0.667,\ -1.139,\ 1.202,\ 0.629,\ 1.086,\ -1.111]
  \]

#### Признак RAD
- Значения: [1, 2, 2, 3, 3, 3]
- Среднее: \(\mu_{\text{RAD}} = 2.3333\)
- СКО: \(\sigma_{\text{RAD}} \approx 0.7454\)
- Z-оценки:
  \[
  z_{\text{RAD}} \approx [-1.7889,\ -0.4472,\ -0.4472,\ 0.8944,\ 0.8944,\ 0.8944]
  \]

Далее работаем с матрицей признаков, где каждая строка — [1, z_CRIM, z_ZN, …, z_LSTAT]. Для вычислений ниже используем только [1, z_CRIM, z_RM, z_RAD] — остальные признаки аналогичны.

---

## Итерация 1: вычисляем градиенты и обновляем веса

Инициализация: все веса равны 0, значит первые предсказания нулевые.

#### 1) Предсказания и ошибки
- Предсказания: \(\hat{y}^{(i)}=0\).
- Ошибки: \(e^{(i)}=\hat{y}^{(i)}-y^{(i)} = -y^{(i)}\).
  \[
  \mathbf{e} = [-24.0,\ -21.6,\ -34.7,\ -33.4,\ -36.2,\ -28.7]
  \]

#### 2) Градиент по bias
\[
\frac{\partial \text{MSE}}{\partial w_0} = \frac{2}{m}\sum e^{(i)} = \frac{2}{6}(-178.6) = -59.5333
\]
Обновление: \(w_0 \leftarrow 0 - 0.01\cdot(-59.5333) = 0.5953\).

#### 3) Градиент по CRIM
\[
\frac{\partial \text{MSE}}{\partial w_{\text{CRIM}}} = \frac{2}{m}\sum e^{(i)} z_{\text{CRIM}}^{(i)}
= \frac{2}{6}\left(-\sum y^{(i)} z_{\text{CRIM}}^{(i)}\right)
\]
Сумма:
- \(24\cdot(-1.3806)=-33.1344\)
- \(21.6\cdot(-0.2535)=-5.482\)
- \(34.7\cdot(-0.2546)=-8.832\)
- \(33.4\cdot 0.0182=0.6079\)
- \(36.2\cdot 1.9877=71.961\)
- \(28.7\cdot(-0.1171)=-3.358\)

Итого \(\sum y z \approx 21.7625\) ⇒ \(\sum e z \approx -21.7625\).

Градиент: \(-\frac{2}{6}\cdot 21.7625 \approx -7.254\)

Обновление: \(w_{\text{CRIM}} \leftarrow 0 - 0.01\cdot(-7.254) \approx 0.0725\).

#### 4) Градиент по RM
Аналогично:
- \(\sum y z_{\text{RM}} \approx 29.545\) ⇒ \(\sum e z_{\text{RM}} \approx -29.545\)
- Градиент: \(\frac{2}{6}(-29.545)\approx -9.848\)
- Обновление: \(w_{\text{RM}} \leftarrow 0 - 0.01\cdot(-9.848) \approx 0.0985\)

#### 5) Градиент по RAD
- \(\sum y z_{\text{RAD}} \approx 19.776\) ⇒ \(\sum e z_{\text{RAD}} \approx -19.776\)
- Градиент: \(\frac{2}{6}(-19.776)\approx -6.592\)
- Обновление: \(w_{\text{RAD}} \leftarrow 0 - 0.01\cdot(-6.592) \approx 0.0659\)

Итого после 1-й итерации (остальные веса пока 0):
- \(w_0 \approx 0.5953\)
- \(w_{\text{CRIM}} \approx 0.0725\)
- \(w_{\text{RM}} \approx 0.0985\)
- \(w_{\text{RAD}} \approx 0.0659\)

#### 6) Предсказания после обновления
Для каждой строки:
\[
\hat{y} \approx w_0 + w_{\text{CRIM}}\,z_{\text{CRIM}} + w_{\text{RM}}\,z_{\text{RM}} + w_{\text{RAD}}\,z_{\text{RAD}}
\]
- Стр.1: \(0.5953 + 0.0725(-1.3806)+0.0985(-0.667)+0.0659(-1.7889)\approx 0.311\)
- Стр.2: \(0.5953 + 0.0725(-0.2535)+0.0985(-1.139)+0.0659(-0.4472)\approx 0.435\)
- Стр.3: \(0.5953 + 0.0725(-0.2546)+0.0985(1.202)+0.0659(-0.4472)\approx 0.666\)
- Стр.4: \(0.5953 + 0.0725(0.0182)+0.0985(0.629)+0.0659(0.8944)\approx 0.717\)
- Стр.5: \(0.5953 + 0.0725(1.9877)+0.0985(1.086)+0.0659(0.8944)\approx 0.905\)
- Стр.6: \(0.5953 + 0.0725(-0.1171)+0.0985(-1.111)+0.0659(0.8944)\approx 0.536\)

Ошибки стали чуть ближе к нулю, но всё ещё большие (мы обновили пока лишь 3 признака; в реальности на этой итерации обновляются все).

---

## Итерация 2: снова градиенты и обновления (на тех же 4 весах для наглядности)

Теперь берём новые ошибки \(e=\hat{y}-y\) из конца итерации 1:

- Стр.1: \(0.311-24=-23.689\)
- Стр.2: \(0.435-21.6=-21.165\)
- Стр.3: \(0.666-34.7=-34.034\)
- Стр.4: \(0.717-33.4=-32.683\)
- Стр.5: \(0.905-36.2=-35.295\)
- Стр.6: \(0.536-28.7=-28.164\)

Далее по формулам:

#### 1) Bias
\[
g_0=\frac{2}{6}\sum e^{(i)}=\frac{2}{6}\cdot(-175.029)\approx -58.343
\]
Обновление: \(w_0 \leftarrow 0.5953 - 0.01\cdot(-58.343)\approx 1.1788\).

#### 2) CRIM
Сумма \(\sum e\cdot z_{\text{CRIM}} \approx -20.629\) ⇒
\[
g_{\text{CRIM}}=\frac{2}{6}(-20.629)\approx -6.876
\]
Обновление: \(w_{\text{CRIM}} \leftarrow 0.0725 - 0.01\cdot(-6.876)\approx 0.1413\).

#### 3) RM
Сумма \(\sum e\cdot z_{\text{RM}} \approx -28.602\) ⇒
\[
g_{\text{RM}}=\frac{2}{6}(-28.602)\approx -9.534
\]
Обновление: \(w_{\text{RM}} \leftarrow 0.0985 - 0.01\cdot(-9.534)\approx 0.1938\).

#### 4) RAD
Сумма \(\sum e\cdot z_{\text{RAD}} \approx -18.917\) ⇒
\[
g_{\text{RAD}}=\frac{2}{6}(-18.917)\approx -6.306
\]
Обновление: \(w_{\text{RAD}} \leftarrow 0.0659 - 0.01\cdot(-6.306)\approx 0.1290\).

После 2-й итерации по этим 4 весам:
- \(w_0 \approx 1.1788\)
- \(w_{\text{CRIM}} \approx 0.1413\)
- \(w_{\text{RM}} \approx 0.1938\)
- \(w_{\text{RAD}} \approx 0.1290\)

Дальше процесс повторяется: на каждой итерации пересчитываются предсказания, ошибки, градиенты — и обновляются все веса.

---

## Что важно понимать

- Я подробно расписал расчёт для bias, CRIM, RM, RAD на двух итерациях. Остальные признаки считаются идентично: берёшь их стандартизированные значения \(z_j^{(i)}\), умножаешь на ошибки, суммируешь, умножаешь на \(\tfrac{2}{m}\) — это градиент по \(w_j\); затем делаешь шаг по \(\alpha\).
- Если сделать обновления сразу по всем 13 признакам, сходимость будет быстрее, и предсказания на каждой итерации будут выше (ближе к реальным значениям).
- На 6 строках и 13 признаках задача плохо обусловлена, поэтому полезно:
  - держать признаки стандартизированными;
  - проверять мультиколлинеарность;
  - при желании добавить L2‑регуляризацию.




стандартное отклонение \( \sigma \) — это мера того, насколько значения в наборе данных **в среднем отклоняются от своего среднего значения**.  

---

## 📐 Формула

Для набора из \( m \) чисел \( x_1, x_2, \dots, x_m \):

1. **Среднее**:
\[
\mu = \frac{1}{m} \sum_{i=1}^m x_i
\]

2. **Дисперсия** (средний квадрат отклонений):
\[
\text{Var} = \frac{1}{m} \sum_{i=1}^m (x_i - \mu)^2
\]

3. **Стандартное отклонение**:
\[
\sigma = \sqrt{\text{Var}} = \sqrt{\frac{1}{m} \sum_{i=1}^m (x_i - \mu)^2}
\]

> Если считать **выборочное** стандартное отклонение (для оценки по выборке, а не по всей генеральной совокупности), то в знаменателе будет \( m-1 \) вместо \( m \).

---

## 🔢 Пример на данных (признак CRIM)

Значения:
\[
[0.00632,\ 0.02731,\ 0.02729,\ 0.03237,\ 0.06905,\ 0.02985]
\]

1. Среднее:
\[
\mu_{\text{CRIM}} = \frac{0.00632 + 0.02731 + 0.02729 + 0.03237 + 0.06905 + 0.02985}{6} \approx 0.032032
\]

2. Квадраты отклонений:
\[
(0.00632 - 0.032032)^2 \approx 0.000660
\]
\[
(0.02731 - 0.032032)^2 \approx 0.000022
\]
\[
(0.02729 - 0.032032)^2 \approx 0.000023
\]
\[
(0.03237 - 0.032032)^2 \approx 0.00000011
\]
\[
(0.06905 - 0.032032)^2 \approx 0.001373
\]
\[
(0.02985 - 0.032032)^2 \approx 0.000005
\]

3. Дисперсия:
\[
\text{Var} = \frac{0.000660 + 0.000022 + 0.000023 + 0.00000011 + 0.001373 + 0.000005}{6} \approx 0.000347
\]

4. Стандартное отклонение:
\[
\sigma_{\text{CRIM}} = \sqrt{0.000347} \approx 0.018624
\]

---

## 📌 Интерпретация
- Если \(\sigma\) маленькое, значения признака близки к среднему.
- Если \(\sigma\) большое, значения сильно разбросаны.

---
</math>

</body>
</html>